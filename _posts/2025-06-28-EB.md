---
layout: single
title: "How to learn hyperparameters via empirical Bayes"
date: 2025-06-28
categories: mathematical think-throughs
---

In a [previous post](2025-06-28-map_vs_empirical_bayes.md) we talked about how we should not learn hyperparameters by naivly maximizing the joint probability of data and parameters. In this post we will focus on the ***Empirical Bayes*** (or ***maximum marginal likelihood***, or ***Type-II maximum likelihood***) method. You might be familiar with the concept of cross validation, which is effective when we have only a few hyperparameters to optimze. But if we have many hyperparameters, for instance, in a hierarchical model, [[maybe think of example]], then a grid search over all combinations of hyperparameters is intractable. It would be nice if we have a differentiable objective as a function of the hyperparameters and do gradient based optimization. A bayesian[^1] form of such objective is called the ***marginal likelihood***. 

 

[^1]: There is debate on how truly *Bayesian* this method is, since it is stil finding a point estimate of the hyperparameter, rather than putting a prior and do fully Bayesian sampling of the hyperparamter. Personally and practically I am not super interested in such debate, as in cases I encounter having a good point estimate of the hyperparameter is useful enough. I view this method as Bayesian because it involves ***marginalization***, which is arguably the core spirit of the Bayesian way of dealing with uncertainty.   
