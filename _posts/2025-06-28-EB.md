---
layout: single
title: "How to learn hyperparameters via empirical Bayes"
date: 2025-06-28
categories: mathematical_think_throughs
---

In a [previous post](https://samdeoxys1.github.io/blog/mathematical_think_throughs/2025/06/28/map_vs_empirical_bayes.html) we talked about how we should not learn hyperparameters by naivly maximizing the joint probability of data and parameters. In this post we will focus on the ***Empirical Bayes*** (or ***maximum marginal likelihood***, or ***Type-II maximum likelihood***) method. You might be familiar with the concept of cross validation, which is effective when we have only a few hyperparameters to optimze. But if we have many hyperparameters, for instance, in a hierarchical model, [[maybe think of example]], then a grid search over all combinations of hyperparameters is intractable. It would be nice if we have a differentiable objective as a function of the hyperparameters and do gradient based optimization. 

### What is marginal likelihood

A bayesian[^1] form of such objective is called the ***marginal likelihood***:
$$
\begin{equation}
p(y|\sigma)=\int p(y|w)p(w|\sigma)dw
\end{equation}
$$
This formula is saying: how likely *on average* it is to observe data $y$ given hyperparameter $\sigma$, where the average is over the prior distribution of the parameter $w$. It is a measure of how good the generative model is for the observed data. 

#### Why is marginal likelihood loved: Occam's razor

One reason why marginal likelihood is estouted by some Bayesians is that it automatically encodes the "occam's razor" property -- a philosophical tenant that among the hypotheses that equally explain the phenomenon, the simplest should win. There is a mathematical way of showing this, but I found an intuitive way:

[[...]]

Therefore, optimizing marginal likelihood is like "selecting" the "best" model --  "best" in the sense that balances fit and parsimony. 



### How to maximize marginal likelihood (tedius derivation alert)

Let's derive the marginal likelihood in the linear regression case (for the multivariate version see [^bishop3 ]). Recall:
$$
\begin{align}
y&=wx+\epsilon \label{forward} &\text{Observation}\\
w&\sim Norm(0,\sigma^2) &\text{Parameter prior}\\
\epsilon&\sim Norm(0,\delta^2) &\text{Noise prior}
\end{align}
$$
The marginal likelihood can be written as:
$$
\begin{align}
p(\{y_i\}|\{x_i\},\sigma)&=\int p(\{y_i\}|\{x_i\},w)p(w|\sigma^2)dw\\
&\sim\int \exp(-\frac{\sum_i(y_i-x_iw)^2}{2\delta^2}-\frac{w^2}{2\sigma^2})dw & \text{omiting normalizing terms in each gaussian density}\\
\end{align}
$$
Here we are using a classic trick in gaussian integrals: we know the marginal distribution of $y$ (conditioned on input and hyperparameter) -- $p(y|x,\sigma)$ is gaussian because from eq. $\ref{forward}$, $wx$ is the scaling of a gaussian $w$ and thus still a gaussian; and the sum of two gaussians remain gaussian. Gaussian is determined by mean and covariance. Both can be read off from the exponents in the pdf. Thus, we can ignore the normalizing terms, i.e. those that are not in the exponents, and focus on the terms within the exponents. Furthermore, whenever we form quadratic terms on the exponent, that term can be integrated out into some normalizing constant. Thus the goal in manipulating gaussian is to form quadratic terms in the exponent, using ***completing the square***. Concretely, here we are taking the integral with respect to $w$. Thus, we want to complete the square with respect to $w$ in the exponent. The first step is to expand the terms:
$$
\begin{align}
-\frac{\sum_i(y_i-x_iw)^2}{2\delta^2}-\frac{w^2}{2\sigma^2}=-\frac{\sum_i y_i^2-\sum_i2x_iy_iw + \sum_i x_i^2w^2}{2\delta^2}-\frac{w^2}{2\sigma^2}\\
\end{align}
$$
Simplify the terms: let's define: $v=\sum_iy_i^2$, $m=\sum_i x_iy_i$, $c=\sum_i x_i^2$ (IMPORTANT notation please keep track!) and rearrange the terms according to the order of $w$:
$$
\begin{align}
-\frac{1}{2}((\frac{1}{\sigma^2}+\frac{c}{\delta^2})w^2-2\frac{m}{\delta^2}w+\frac{v}{\delta^2}) \label{pre complete square}
\end{align}
$$
Recall that to complete the squares for $aw-2bw$, we get:
$$
\begin{align}
aw-2bw&=a(w-(2b/a)w + (b/a)^2- (b/a)^2) \\
&=a(w-(b/a))^2- b^2/a \label{complete square formula}\\
\end{align}
$$
Apply eq. $\ref{complete square formula}$ to eq. $\ref{pre complete square}$, we don't even need to write out the full quadratic term, since the quadratic exponential of $w$ will be integrated over. The only terms left are the ones that don't involve $w$ and can be taken out of the integral, i.e. $-b^2/a$, where $b=m/\delta^2$, $a=(\frac{1}{\sigma^2}+\frac{c}{\delta^2})$, and thus $-b^2/a=-\frac{m^2}{\delta^4}/(\frac{1}{\sigma^2}+\frac{c}{\delta^2})$. Note also that the $v/\delta^2$ term in eq. $\ref{pre complete square}$ doesn't involve $w$ and its exponential can be taken out of the integral. Together, we are left with:
$$
\begin{align}
p(\{y_i\}|\{x_i\},\sigma)\sim \exp(-\frac{1}{2}[\frac{v}{\delta^2}-\frac{m^2}{\delta^4(\frac{1}{\sigma^2}+\frac{c}{\delta^2})}])
\end{align}
$$
Notice: this is a multivariate gaussian density of $\{y_i\}$. $y$ is hiding in $v$ and $m$ and only has quadratic terms, thus we know $y$ has $0$ mean. 

Finally, we can optimize the marginal likelihood with respect to $\sigma^2$ to get (steps omitted):
$$
\begin{align}
\hat\sigma^2
=\frac{m^2 - c\,\delta^2}{c^2}
=\frac{(\sum_i x_i y_i)^2 - \delta^2\sum_i x_i^2}
     {\bigl(\sum_i x_i^2\bigr)^2}\,. \label{eb_sigma}
\end{align}
$$
Remarkably, we can express this solution of the prior varaince in terms of the maximum likelihood and maximum a posteriori solution of the weights. First recall that: 
$$
\begin{align}
\hat w_{ML}&=\frac{m}{c} \label{ml}\\
\hat w_{MAP}&=\frac{\sigma^2m}{\delta^2+\sigma^2c} \label{map}\\
\end{align}
$$
the relationship is so elegant and not easy to see at first sight. Try it for yourself. It's satisfying!
$$
\begin{align}
\hat \sigma^2 = \hat w_{ML}\hat w_{MAP}
\end{align}
$$
Here is the derivation:


$$
\begin{align}
c^2\hat\sigma^2 &= m^2 - c\delta^2 & \text{from eq. \ref{eb_sigma}} \label{csq_sigmasq}\\
\hat w_{MAP}(\delta^2+\sigma^2c) &= \hat\sigma^2m & \text{from eq. \ref{map}} \\
\hat w_{MAP}(c\delta^2+\sigma^2c^2) &= \hat\sigma^2mc & \text{multiply both sides by $c$} \\
\hat w_{MAP}(c\delta^2+m^2-c\delta^2) &= \hat\sigma^2mc & \text{use eq.$\ref{csq_sigmasq}$} \\
\hat w_{MAP}m^2/(mc) &= \hat\sigma^2 & \text{divide both sides by mc}\\
\hat w_{MAP}\hat w_{ML} &= \hat\sigma^2 & \text{divide both sides by mc}\\
\end{align}
$$
Thus the empirical-Bayes estimator of prior variance $\sigma$ is the geometric mean of the maximum likelihood estimator and the maximum a posteriori estimator of the parameter $w$![^multiDcaution]



 











 





 



#### 





When we live in the gaussian world, the marginal likelihood has a tractable formula. However, most real world problems don't have this luxury. Computing this integral via simple monte carlo also would not work in most cases [[more explanation?]] Thus we resort to variational inference [[]]

the math...



code (demo learning a poisson glm)...







 

[^1]: There is debate on how truly *Bayesian* this method is, since it is stil finding a point estimate of the hyperparameter, rather than putting a prior and do fully Bayesian sampling of the hyperparamter. Personally and practically I am not super interested in such debate, as in cases I encounter having a good point estimate of the hyperparameter is useful enough. I view this method as Bayesian because it involves ***marginalization***, which is arguably the core spirit of the Bayesian way of dealing with uncertainty.   
[^bishop3]: Bishop, C.â€‰M. (2006). *Pattern Recognition and Machine Learning*, Chapter 3. Springer.
[^multiDcaution]:  This relation only holds in linear regression where dependent variable is 1D.



