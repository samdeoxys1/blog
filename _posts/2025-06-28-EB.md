---
layout: single
title: "How to learn hyperparameters via empirical Bayes"
date: 2025-06-28
categories: mathematical_think_throughs
---

In a [previous post](https://samdeoxys1.github.io/blog/mathematical_think_throughs/2025/06/28/map_vs_empirical_bayes.html) we talked about how we should not learn hyperparameters by naivly maximizing the joint probability of data and parameters. In this post we will focus on the ***Empirical Bayes*** (or ***maximum marginal likelihood***, or ***Type-II maximum likelihood***) method. You might be familiar with the concept of cross validation, which is effective when we have only a few hyperparameters to optimze. But if we have many hyperparameters, for instance, in a hierarchical model, [[maybe think of example]], then a grid search over all combinations of hyperparameters is intractable. It would be nice if we have a differentiable objective as a function of the hyperparameters and do gradient based optimization. 

### What is marginal likelihood

A bayesian[^1] form of such objective is called the ***marginal likelihood***:
$$
\begin{equation}
p(y|\sigma)=\int p(y|w)p(w|\sigma)dw
\end{equation}
$$
This formula is saying: how likely *on average* it is to observe data $y$ given hyperparameter $\sigma$, where the average is over the prior distribution of the parameter $w$. It is a measure of how good the generative model is for the observed data. 

#### Why is marginal likelihood loved: Occam's razor

One reason why marginal likelihood is estouted by some Bayesians is that it automatically encodes the "occam's razor" property -- a philosophical tenant that among the hypotheses that equally explain the phenomenon, the simplest should win. There is a mathematical way of showing this, but I found an intuitive way:

[[...]]

Therefore, optimizing marginal likelihood is like "selecting" the "best" model --  "best" in the sense that balances fit and parsimony. 



### How to maximize marginal likelihood: in non-toy cases

When we live in the gaussian world, the marginal likelihood has a tractable formula. However, most real world problems don't have this luxury. Computing this integral via simple monte carlo also would not work in most cases [[more explanation?]] Thus we resort to variational inference [[]]

the math...



code (demo learning a poisson glm)...







 

[^1]: There is debate on how truly *Bayesian* this method is, since it is stil finding a point estimate of the hyperparameter, rather than putting a prior and do fully Bayesian sampling of the hyperparamter. Personally and practically I am not super interested in such debate, as in cases I encounter having a good point estimate of the hyperparameter is useful enough. I view this method as Bayesian because it involves ***marginalization***, which is arguably the core spirit of the Bayesian way of dealing with uncertainty.   