---
layout: single
title: "How to learn hyperparameters via empirical Bayes"
date: 2025-06-28
categories: mathematical_think_throughs
---

In a [previous post](https://samdeoxys1.github.io/blog/mathematical_think_throughs/2025/06/28/map_vs_empirical_bayes.html) we talked about how we should not learn hyperparameters by naivly maximizing the joint probability of data and parameters. In this post we will focus on the ***Empirical Bayes*** (or ***maximum marginal likelihood***, or ***Type-II maximum likelihood***) method. You might be familiar with the concept of cross validation, which is effective when we have only a few hyperparameters to optimze. But if we have many hyperparameters, for instance, in a hierarchical model, [[maybe think of example]], then a grid search over all combinations of hyperparameters is intractable. It would be nice if we have a differentiable objective as a function of the hyperparameters and do gradient based optimization. 

### What is marginal likelihood

A bayesian[^1] form of such objective is called the ***marginal likelihood***:
$$
\begin{equation}
p(y|\sigma)=\int p(y|w)p(w|\sigma)dw
\end{equation}
$$
This formula is saying: how likely *on average* it is to observe data $y$ given hyperparameter $\sigma$, where the average is over the prior distribution of the parameter $w$. It is a measure of how good the generative model is for the observed data. 

#### Why is marginal likelihood loved: Occam's razor

One reason why marginal likelihood is estouted by some Bayesians is that it automatically encodes the "occam's razor" property -- a philosophical tenant that among the hypotheses that equally explain the phenomenon, the simplest should win. There is a mathematical way of showing this (see Mackay[^mackay]), but I prefer a more intuitive way. Suppose we are doing simple monte carlo to estimate the marginal likelihood[^ smc warning]:
$$
\begin{align}
p(y|\sigma)&\approx\frac{1}{N}\sum_{i=1}^{N} p(y|w_i) & w_i\sim p(w|\sigma). 
\end{align}
$$
When working with probablistic models, it is a must to work in log space to avoid numerical issues:
$$
\begin{align}
\log p(y|\sigma)&\approx \log \sum_{i=1}^{N}\exp(\log p(y|w_i)) - \log N \\
&\approx \text{logsumexp}_i \log p(y|w_i) - \log N \label{lml}
\end{align}
$$

#### a detour on logsumexp (can skip if you are familiar)

The weird "logsumexp" denotes literally "taking the log of the sum of a bunch of exponentials". A name is given to this operation because it is a common operation but should not be performed naively, since the exponential of really small log will lead to numerical underflow (This is however not explained  in standard textbooks!!). The trick is: 
$$
\begin{align}
\text{logsumexp}_i k_i&=\text{logsumexp}(k_1,..k_N)\\
&=\log\sum_i \exp(k_{max})\exp (k_i - k_{max})\\
&=\log \exp(k_{max})\sum_i \exp (k_i - k_{max}) \\
&=\log \exp(k_{max})+\log \sum_i \exp (k_i - k_{max})\\
&=k_{max}+\log \sum_i \exp (k_i - k_{max}) \label{logsumexp_result}\\
&=k_{max}+\log (1+\text{terms close to 0}) \\
& \text{1 from $k_{max}$, close to 0 from exponential of large negative numbers}\\  
&\approx k_{max}
\end{align}
$$
Thus "logsumexp" can be viewed as "softmax"[^ correct softmax]. (In practice we use the logsumexp function in scipy.special.)

#### back to what marginal likelihood means

Thus eq. $\ref{lml}$ can be written as:
$$
\begin{align}
\log p(y|\sigma)&\approx \log p(y|w_{best sample})-\log N,
\end{align}
$$
i.e. a **model fit term** -- the best likelihood achieved by the samples $\log p(y|w_{best sample})$ and a **complexity penalty term** -- $-\log N$. Although theoretically a more complex model could achieve a higher likelihood. But if the "good" parameters (good meaning achieving high likelihood) concentrate within a small region, then one would need a lot of prior samples to hit that region. This situation is often worse in complex models, where the prior volumne is so big that the posterior volumne only occupies a tiny fraction of the prior. Thus for a complex model to hit a good region in the parameter space via prior sampling, it would require much more samples than the simpler models. 

***Therefore, optimizing marginal likelihood is like "selecting" the "best" model --  "best" in the sense that balances fit and parsimony.*** 



### How to maximize marginal likelihood (tedius derivation alert)

Let's derive the marginal likelihood in the linear regression case (for the multivariate version see [^bishop3 ]). The reward of the tedius derivation is a remarkable formula linking the empirical-Bayes estimator of the prior variance in terms of the maximum likelihood and maximum a posteriori estimator of the weights in linear regression in 1D:
$$
\begin{align}
\hat \sigma^2 = \hat w_{ML}\hat w_{MAP}
\end{align}
$$


Recall:
$$
\begin{align}
y&=wx+\epsilon \label{forward} &\text{Observation}\\
w&\sim Norm(0,\sigma^2) &\text{Parameter prior}\\
\epsilon&\sim Norm(0,\delta^2) &\text{Noise prior}
\end{align}
$$
The marginal likelihood can be written as:
$$
\begin{align}
p(\{y_i\}|\{x_i\},\sigma)&=\int p(\{y_i\}|\{x_i\},w)p(w|\sigma^2)dw\\
&\sim\int \exp(-\frac{\sum_i(y_i-x_iw)^2}{2\delta^2}-\frac{w^2}{2\sigma^2})dw & \text{omiting normalizing terms in each gaussian density}\\
\end{align}
$$
Here we are using a classic trick in gaussian integrals: we know the marginal distribution of $y$ (conditioned on input and hyperparameter) -- $p(y|x,\sigma)$ is gaussian because from eq. $\ref{forward}$, $wx$ is the scaling of a gaussian $w$ and thus still a gaussian; and the sum of two gaussians remain gaussian. Gaussian is determined by mean and covariance. Both can be read off from the exponents in the pdf. Thus, we can ignore the normalizing terms, i.e. those that are not in the exponents, and focus on the terms within the exponents. Furthermore, whenever we form quadratic terms on the exponent, that term can be integrated out into some normalizing constant. Thus the goal in manipulating gaussian is to form quadratic terms in the exponent, using ***completing the square***. Concretely, here we are taking the integral with respect to $w$. Thus, we want to complete the square with respect to $w$ in the exponent. The first step is to expand the terms:
$$
\begin{align}
-\frac{\sum_i(y_i-x_iw)^2}{2\delta^2}-\frac{w^2}{2\sigma^2}=-\frac{\sum_i y_i^2-\sum_i2x_iy_iw + \sum_i x_i^2w^2}{2\delta^2}-\frac{w^2}{2\sigma^2}\\
\end{align}
$$
Simplify the terms: let's define: $v=\sum_iy_i^2$, $m=\sum_i x_iy_i$, $c=\sum_i x_i^2$ (IMPORTANT notation please keep track!) and rearrange the terms according to the order of $w$:
$$
\begin{align}
-\frac{1}{2}((\frac{1}{\sigma^2}+\frac{c}{\delta^2})w^2-2\frac{m}{\delta^2}w+\frac{v}{\delta^2}) \label{pre complete square}
\end{align}
$$
Recall that to complete the squares for $aw-2bw$, we get:
$$
\begin{align}
aw-2bw&=a(w-(2b/a)w + (b/a)^2- (b/a)^2) \\
&=a(w-(b/a))^2- b^2/a \label{complete square formula}\\
\end{align}
$$
Apply eq. $\ref{complete square formula}$ to eq. $\ref{pre complete square}$, we don't even need to write out the full quadratic term, since the quadratic exponential of $w$ will be integrated over. The only terms left are the ones that don't involve $w$ and can be taken out of the integral, i.e. $-b^2/a$, where $b=m/\delta^2$, $a=(\frac{1}{\sigma^2}+\frac{c}{\delta^2})$, and thus $-b^2/a=-\frac{m^2}{\delta^4}/(\frac{1}{\sigma^2}+\frac{c}{\delta^2})$. Note also that the $v/\delta^2$ term in eq. $\ref{pre complete square}$ doesn't involve $w$ and its exponential can be taken out of the integral. Together, we are left with:
$$
\begin{align}
p(\{y_i\}|\{x_i\},\sigma)\sim \exp(-\frac{1}{2}[\frac{v}{\delta^2}-\frac{m^2}{\delta^4(\frac{1}{\sigma^2}+\frac{c}{\delta^2})}])
\end{align}
$$
Notice: this is a multivariate gaussian density of $\{y_i\}$. $y$ is hiding in $v$ and $m$ and only has quadratic terms, thus we know $y$ has $0$ mean. 

Finally, we can optimize the marginal likelihood with respect to $\sigma^2$ to get (steps omitted):
$$
\begin{align}
\hat\sigma^2
=\frac{m^2 - c\,\delta^2}{c^2}
=\frac{(\sum_i x_i y_i)^2 - \delta^2\sum_i x_i^2}
     {\bigl(\sum_i x_i^2\bigr)^2}\,. \label{eb_sigma}
\end{align}
$$
Remarkably, we can express this solution of the prior varaince in terms of the maximum likelihood and maximum a posteriori solution of the weights. First recall that: 
$$
\begin{align}
\hat w_{ML}&=\frac{m}{c} \label{ml}\\
\hat w_{MAP}&=\frac{\sigma^2m}{\delta^2+\sigma^2c} \label{map}\\
\end{align}
$$
the relationship is so elegant and not easy to see at first sight. Try it for yourself. It's satisfying!
$$
\begin{align}
\hat \sigma^2 = \hat w_{ML}\hat w_{MAP}
\end{align}
$$
Here is the derivation:


$$
\begin{align}
c^2\hat\sigma^2 &= m^2 - c\delta^2 & \text{from eq. \ref{eb_sigma}} \label{csq_sigmasq}\\
\hat w_{MAP}(\delta^2+\sigma^2c) &= \hat\sigma^2m & \text{from eq. \ref{map}} \\
\hat w_{MAP}(c\delta^2+\sigma^2c^2) &= \hat\sigma^2mc & \text{multiply both sides by $c$} \\
\hat w_{MAP}(c\delta^2+m^2-c\delta^2) &= \hat\sigma^2mc & \text{use eq.$\ref{csq_sigmasq}$} \\
\hat w_{MAP}m^2/(mc) &= \hat\sigma^2 & \text{divide both sides by mc}\\
\hat w_{MAP}\hat w_{ML} &= \hat\sigma^2 & \text{divide both sides by mc}\\
\end{align}
$$
Thus the empirical-Bayes estimator of prior variance $\sigma$ is the geometric mean of the maximum likelihood estimator and the maximum a posteriori estimator of the parameter $w$![^multiDcaution]



### Next time, the real world...

However, most real world problems don't have the luxury of a closed form marginal likelihood! Next time we will talk about how to use variational inference to do empirical Bayes when the marginal likelihood is analytically intractable. 



[^1]: There is debate on how truly *Bayesian* this method is, since it is stil finding a point estimate of the hyperparameter, rather than putting a prior and do fully Bayesian sampling of the hyperparamter. Personally and practically I am not super interested in such debate, as in cases I encounter having a good point estimate of the hyperparameter is useful enough. I view this method as Bayesian because it involves ***marginalization***, which is arguably the core spirit of the Bayesian way of dealing with uncertainty.   



[^bishop3]: Bishop, C.â€‰M. (2006). *Pattern Recognition and Machine Learning*, Chapter 3. Springer.



[^multiDcaution]:  This relation only holds in linear regression where dependent variable is 1D.



[^mackay ]: MacKay, David JC. *Information theory, inference and learning algorithms*. Cambridge university press, 2003.



[^ smc warning]: Simple monte carlo is NOT a recommended way of estimating the marginal likelihood beyond a handful of dimensions. Here we only use it for intuition.  



[^ correct softmax]: The *softmax* common in deep learning should be called "*soft arg-max*"-- From Alfredo Canziani