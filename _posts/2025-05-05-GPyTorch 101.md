---
layout: post
title: "GPyTorch 101"
date: 2025-05-05
categories: tutorial notebooks
---

Recently I have been using AI to quickly prototype some complex Gaussian Process (GP) related models in GPytorch. While it does allow extremely fast prototyping, the workflow with prompting, testing and re-prompting often leave me this empty and anxious feeling, which is the opposite of the good solid feeling of building a model from ground up, line by line. So I decide to slow down and learn the basic building blocks of GPytorch. 

Another motivation for the notebook is that I want to try a new habit for learning. My old habit of learning is an iterative process of trying new models and understanding the math. However, when the new methods I tried did not make its way into my project, I moved on. Even though consciously I knew I still learned something, but the lack of tangible progress also added to the feeling of emptiness and anxiety. So instead of the endless chase of the final thing that works, I decide to document the journey more. Forcing myself to pause is not easy, when the brain is in the habit of trying the next tweak in the name of a deeper understanding. (This habit also makes paper writing harder to start.) But the pause is actually useful in many ways, as the chase often becomes mechanical (e.g. change a hyperparameter and press run and wait) and inefficient at the end of the day, while pausing and taking a step back creates the space my brain needs to see the fuller picture.  

With that said, in this notebook, I will to train the simplest Gaussian Process (GP) Regression model in GPytorch.


```python
import gpytorch
import torch
import matplotlib.pyplot as plt
import numpy as np
if torch.cuda.is_available():
    # 1. Pick your GPU (usually 0 if you have just one)
    torch.cuda.set_device(0)
    # 2. Make FloatTensors and DoubleTensors default to CUDA

    torch.set_default_dtype(torch.float32)   # optional, ensures float32
else:
    print("CUDA not available, falling back to CPU.")
    
# a helper function to turn the tensor on gpu to numpy on cpu
def strip(*args):
    to_return = tuple((x.detach().cpu().numpy() for x in args))
    if len(args)==1:
        return to_return[0]
    else:
        return to_return

```

First, to define a gpytorch model, we subclass some model from the gpytorch.models. For ExactGP, we need to initialize with the training x and y, as well as the likelihood. Next we need to assign the mean_module and covar_module, which are the mean and kernel functions that define the GP. Just like the regular pytorch models, it requires a forward method, which maps the input to the distribution of the output function evaluated at the input. The kernel here is a composition of ScaleKernel and RBFKernel. The former simply adds an output scale to the RBFKernel. This style of syntax highlights the compositionality in GPytorch, which is very principled but can also be a bit cumbersome at times.  


```python
class ExactGPModel(gpytorch.models.ExactGP):
    def __init__(self,train_x,train_y,likelihood):
        super().__init__(train_x,train_y,likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())
        
    def forward(self,x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x,covar_x)
        
```

We will be generate some data from a ground truth ("teacher") GP and fit a "student" GP to see if we can recover the ground truth hyperparameters.  

We initialize the teacher GP with training data being None, together with the likelihood (which sets the observation noise). We use .initialize(hyperparam=xxx) to set relevant hyperparameters. 



```python
import math
base_x = torch.linspace(0,1,10)
base_y = torch.sin(base_x*2*math.pi)
```


```python
likelihood_true = gpytorch.likelihoods.GaussianLikelihood()
likelihood_true.noise_covar.initialize(noise=.01)
model_true=ExactGPModel(None,None,likelihood_true)
lengthscale_teacher=0.01
model_true.covar_module.base_kernel.initialize(lengthscale=lengthscale_teacher)
model_true.covar_module.initialize(outputscale=1)



```




    ScaleKernel(
      (base_kernel): RBFKernel(
        (raw_lengthscale_constraint): Positive()
      )
      (raw_outputscale_constraint): Positive()
    )



Now we generate the actual training data using the teacher GP: evaluate the GP at some query points and sample. 



```python
train_x = torch.linspace(0, 1, 1000)
```


```python
model_true.eval()
train_y = likelihood_true(model_true(train_x)).sample()
```

Next, we initialize the student GP. In some cases we might not want to learn all hyperparameters, since the model could learn to increase one and decrease another to achieve the same fit (i.e. unidentifiability). We do this by doing model.covar_module.raw_outputscale.requires_grad_(False). Notice the "raw" here. Often the (hyper)parameters are constrained to be positive via some transform, but the optimization is done on the pre-transformed, unconstrained, "raw" (hyper)parameters. Conveniently, to initialize we can directly set the transformed (hyper)parameters.


```python
# student GP; init with the actual train test data
likelihood = gpytorch.likelihoods.GaussianLikelihood()
model = ExactGPModel(train_x,train_y,likelihood)


model.covar_module.base_kernel.initialize(lengthscale=1.)

likelihood.noise_covar.initialize(noise=1.)


model.covar_module.initialize(outputscale=1)
model.covar_module.raw_outputscale.requires_grad_(False)

```




    Parameter containing:
    tensor(0.5413)



To train, first toggle the .train for model **and likelihood**. Gather the trainable parameters in both model and likelihood, and set up the loss from gpytorch.mlls. The rest is the same as normal pytorch!


```python
from itertools import chain
model.train()
likelihood.train()

optimizer =torch.optim.Adam(chain(model.parameters(),likelihood.parameters()),lr=0.1)
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,model)

training_iter = 50
lengthscale_history = []
loss_history=[]
for i in range(training_iter):
    optimizer.zero_grad()
    output = model(train_x)
    loss = -mll(output,train_y)
    loss.backward()
    print(f'Iter {i}, loss {loss.item()},lengthscale {model.covar_module.base_kernel.lengthscale.item()}, noise {likelihood.noise.item()}')                  
    lengthscale_history.append(model.covar_module.base_kernel.lengthscale.item())
    loss_history.append(loss.item())
    optimizer.step()
lengthscale_history=np.array(lengthscale_history)
```

    Iter 0, loss 1.5034751892089844,lengthscale 1.0, noise 1.0
    Iter 1, loss 1.4980993270874023,lengthscale 0.9379605650901794, noise 1.1309800148010254
    Iter 2, loss 1.4988948106765747,lengthscale 0.8797849416732788, noise 1.2386475801467896
    Iter 3, loss 1.5029336214065552,lengthscale 0.8234867453575134, noise 1.2764174938201904
    Iter 4, loss 1.4999040365219116,lengthscale 0.772454023361206, noise 1.252253532409668
    Iter 5, loss 1.495342493057251,lengthscale 0.7225261926651001, noise 1.1975077390670776
    Iter 6, loss 1.4957207441329956,lengthscale 0.673803448677063, noise 1.1330970525741577
    Iter 7, loss 1.4958709478378296,lengthscale 0.6267536878585815, noise 1.079891324043274
    Iter 8, loss 1.4961491823196411,lengthscale 0.5814626812934875, noise 1.0506325960159302
    Iter 9, loss 1.4907739162445068,lengthscale 0.5384774804115295, noise 1.05614173412323
    Iter 10, loss 1.4889053106307983,lengthscale 0.4981774687767029, noise 1.0862661600112915
    Iter 11, loss 1.4896503686904907,lengthscale 0.4612460732460022, noise 1.1324186325073242
    Iter 12, loss 1.4903520345687866,lengthscale 0.4281083941459656, noise 1.1773699522018433
    Iter 13, loss 1.4892841577529907,lengthscale 0.3996175527572632, noise 1.20736825466156
    Iter 14, loss 1.4878891706466675,lengthscale 0.3737684488296509, noise 1.205428123474121
    Iter 15, loss 1.4872599840164185,lengthscale 0.3480561375617981, noise 1.1840044260025024
    Iter 16, loss 1.485624074935913,lengthscale 0.32214224338531494, noise 1.143410563468933
    Iter 17, loss 1.4782428741455078,lengthscale 0.2970481812953949, noise 1.0954828262329102
    Iter 18, loss 1.4751523733139038,lengthscale 0.2737049162387848, noise 1.0565412044525146
    Iter 19, loss 1.4647730588912964,lengthscale 0.2519865930080414, noise 1.026340365409851
    Iter 20, loss 1.454779028892517,lengthscale 0.23108598589897156, noise 1.0107262134552002
    Iter 21, loss 1.442551612854004,lengthscale 0.2108956128358841, noise 0.9966393709182739
    Iter 22, loss 1.4364632368087769,lengthscale 0.19155758619308472, noise 0.9785724878311157
    Iter 23, loss 1.4256380796432495,lengthscale 0.1734786033630371, noise 0.9619303941726685
    Iter 24, loss 1.4172542095184326,lengthscale 0.1569460779428482, noise 0.9533473253250122
    Iter 25, loss 1.414056420326233,lengthscale 0.14193236827850342, noise 0.9483888149261475
    Iter 26, loss 1.408406376838684,lengthscale 0.1282186359167099, noise 0.9412543773651123
    Iter 27, loss 1.3990925550460815,lengthscale 0.11558150500059128, noise 0.9289875030517578
    Iter 28, loss 1.3822308778762817,lengthscale 0.1039513424038887, noise 0.9168523550033569
    Iter 29, loss 1.3786089420318604,lengthscale 0.09319572895765305, noise 0.8888603448867798
    Iter 30, loss 1.3533095121383667,lengthscale 0.08317877352237701, noise 0.8567999005317688
    Iter 31, loss 1.3277922868728638,lengthscale 0.07395485788583755, noise 0.8043214678764343
    Iter 32, loss 1.294185996055603,lengthscale 0.06551481038331985, noise 0.734647810459137
    Iter 33, loss 1.2630661725997925,lengthscale 0.0578378401696682, noise 0.6527823805809021
    Iter 34, loss 1.227391004562378,lengthscale 0.05090642720460892, noise 0.5848780870437622
    Iter 35, loss 1.187210202217102,lengthscale 0.04467599838972092, noise 0.5429995059967041
    Iter 36, loss 1.1100720167160034,lengthscale 0.03916965797543526, noise 0.4996449649333954
    Iter 37, loss 1.0125192403793335,lengthscale 0.034293219447135925, noise 0.43320709466934204
    Iter 38, loss 0.8855133056640625,lengthscale 0.030008766800165176, noise 0.3515050709247589
    Iter 39, loss 0.7406270503997803,lengthscale 0.026213329285383224, noise 0.26954585313796997
    Iter 40, loss 0.5722865462303162,lengthscale 0.022827083244919777, noise 0.19804978370666504
    Iter 41, loss 0.3921799957752228,lengthscale 0.019821971654891968, noise 0.1409120112657547
    Iter 42, loss 0.17829559743404388,lengthscale 0.017167018726468086, noise 0.09782585501670837
    Iter 43, loss -0.08078716695308685,lengthscale 0.014831479638814926, noise 0.06664679199457169
    Iter 44, loss -0.3248673677444458,lengthscale 0.012797893956303596, noise 0.04476097226142883
    Iter 45, loss -0.5276607275009155,lengthscale 0.011114010587334633, noise 0.029737534001469612
    Iter 46, loss -0.6837313771247864,lengthscale 0.00979149155318737, noise 0.019633105024695396
    Iter 47, loss -0.855189859867096,lengthscale 0.008792515844106674, noise 0.012951954267919064
    Iter 48, loss -1.0230518579483032,lengthscale 0.008063042536377907, noise 0.00857518520206213
    Iter 49, loss -1.1039509773254395,lengthscale 0.007552627939730883, noise 0.00570988655090332



```python
fig,axs=plt.subplots(1,2,figsize=(5,2))
ax=axs[0]
ax.plot(loss_history)
ax.set_title('Loss')
ax=axs[1]
ax.plot(lengthscale_history)
ax.set_title('Lengthscale')
ax.axhline(lengthscale_teacher,label='lengthscale_teacher',c='k')
ax.legend(bbox_to_anchor=[1.05,1])
ax.set_xlabel('Training iter.')
plt.tight_layout()
```


    
![png]({{ site.baseurl }}/images/gpytorch_101/output_14_0.png)
    


Lengthscale is correctly recovered!

(You might notice the loss does not seem to converge here and wonder why I don't train for longer. That is becuase when I tried to train for longer, I inevitably run into some warning about conjugate gradient having abnormal norm, and the loss will jump to super huge. I don't understand what's going on...)

To make predictions, **we need to call model_true.eval() here!** Under the .train() mode, the forward pass gives the *prior* distribution of the output, whereas .eval() gives the *posterior* distribution of the output! This is a big difference with usual pytorch models, where the change from .train to .eval might not be so huge. 


```python
model.eval()
with torch.no_grad(), gpytorch.settings.fast_pred_var():
    y_hat = likelihood(model(train_x))
    y_hat_mean=y_hat.mean
    lower,upper=y_hat.confidence_region()


train_x_np=strip(train_x)
train_y_np = strip(train_y)
y_hat_mean_np = strip(y_hat_mean)
lower,upper = strip(lower,upper)
```


```python
fig,ax=plt.subplots()
ax.plot(train_x_np,train_y_np,'k*',alpha=1.,ms=1,label='Data')
ax.plot(train_x_np,y_hat_mean_np,label='Pred. mean')
ax.fill_between(train_x_np,lower,upper,alpha=.5,label='Pred. CI')
ax.legend()
```




    <matplotlib.legend.Legend at 0x15541fa59df0>




    
![png]({{ site.baseurl }}/images/gpytorch_101/output_18_1.png)
    


Isn't this beautiful?


```python

```
